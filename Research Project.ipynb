{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6491253",
   "metadata": {},
   "source": [
    "<h1>Text Classification on Coronavirus tweets</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8336ab",
   "metadata": {},
   "source": [
    "Natural language processing(NLP) in simple words can be defined as the branch of Artificial Intelligence, that gives machines the ability to read, understand and make sense of human languages. Sentiment analysis is also known as opinion mining or AI emotion is the use of natural language processing techniques for analyzing texts and extracting and classifying the sentiments of these texts as positive negative or neutral.<br>\n",
    "Now an interesting question about this type of project that may arise in your mind is that why sentiment analysis on COVID-19 Tweets? What is about the coronavirus tweets that would be positive?<br>\n",
    "The use of social media for communication during the time of crisis has increased remarkably over the recent years. As mentioned above, analyzing social media data is important as it helps understand public sentiment. During the coronavirus pandemic, many people took to social media to express their anger, grief, or sadness while some also spread happiness and positivity. People also used social media to ask their network for help related to vaccines or hospitals during this hard time. Many issues related to this pandemic can also be solved if experts considered this social data. That’s the reason why analyzing this type of data is important to understand the overall issues faced by people.<br>\n",
    "We are going to perform Text Classification on the data. The tweets have been pulled from Twitter and manual tagging has been done then.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41005a0c",
   "metadata": {},
   "source": [
    "__Importing all the necessary libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b16005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import nltk \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0397d2",
   "metadata": {},
   "source": [
    "__Importing Data__<br>\n",
    "For this project, I have used the datasets from Kaggle.The dataset consists of 41157 tweets. There are 5 sentiments namely ‘Positive’, ‘Extremely Positive’, ‘Negative’, ‘Extremely Negative’, and ‘Neutral’ in the sentiment column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79aae089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Corona_NLP_train.csv\",encoding='latin1')\n",
    "data = data[['OriginalTweet', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be37f4",
   "metadata": {},
   "source": [
    "The dataset consists of the ‘UserName’, ‘ScreenName’, ‘Location’, ‘TweetAt’, ‘OriginalTweet’, and ‘Sentiment’ columns. I have dropped the unnecessary columns that are not useful for what we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73129c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1  advice Talk to your neighbours family to excha...            Positive\n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3  My food stock is not the only one which is emp...            Positive\n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629c667",
   "metadata": {},
   "source": [
    "__Understanding the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d42dc961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dbe5f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41157 entries, 0 to 41156\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   OriginalTweet  41157 non-null  object\n",
      " 1   Sentiment      41157 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 643.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf6d9d",
   "metadata": {},
   "source": [
    "From information above we can see that there no null values and both the columns have object datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6808bce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7713\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4802743",
   "metadata": {},
   "source": [
    "The distribution of positive and negative sentiments seems pretty close i.e. 28%, 24% respectively while 16% of tweets are extremely positive and 13% are extremely negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4371f3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "   Sentiment_num  \n",
       "0              1  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting the categorical target variable to numerical target variable\n",
    "target_variable={'Extremely Negative':0, 'Negative':0, 'Neutral':1,\n",
    "                'Positive':2, 'Extremely Positive':2}\n",
    "data['Sentiment_num']=data['Sentiment'].map(lambda x:target_variable[x])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034756da",
   "metadata": {},
   "source": [
    "There is not much difference between ‘Extreme Positive’ and ‘Positive’ and ‘Extremely Negative’ and ‘Negative’, therefore I have replaced extremely positive with positive sentiment and extremely negative as negative and mapped ‘Positive’ sentiment to 1, ‘Negative’ sentiment to 2, and ‘Neutral’ sentiment to 0. This will also help in fast processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a3760",
   "metadata": {},
   "source": [
    "<h1>Data Preprocessing<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520fb80",
   "metadata": {},
   "source": [
    "The tweets data that we get from the API is unstructured and in different languages. This is not convenient for Machine Learning or statistical analysis. Therefore, data preprocessing is an extremely important step as it affects the ability of our model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b4d157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv  and  and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "   Sentiment_num                                               text  \n",
       "0              1         @MeNyrbie @Phil_Gahan @Chrisitv  and  and   \n",
       "1              2  advice Talk to your neighbours family to excha...  \n",
       "2              2  Coronavirus Australia: Woolworths to give elde...  \n",
       "3              2  My food stock is not the only one which is emp...  \n",
       "4              0  Me, ready to go at supermarket during the #COV...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)\n",
    "data['text']=data['OriginalTweet'].apply(lambda x:remove_urls(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d44524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv  and  and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "   Sentiment_num                                               text  \n",
       "0              1         @MeNyrbie @Phil_Gahan @Chrisitv  and  and   \n",
       "1              2  advice Talk to your neighbours family to excha...  \n",
       "2              2  Coronavirus Australia: Woolworths to give elde...  \n",
       "3              2  My food stock is not the only one which is emp...  \n",
       "4              0  Me, ready to go at supermarket during the #COV...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "data['text']=data['text'].apply(lambda x:remove_html(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d01679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>@menyrbie @phil_gahan @chrisitv  and  and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>coronavirus australia: woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>me, ready to go at supermarket during the #cov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "   Sentiment_num                                               text  \n",
       "0              1         @menyrbie @phil_gahan @chrisitv  and  and   \n",
       "1              2  advice talk to your neighbours family to excha...  \n",
       "2              2  coronavirus australia: woolworths to give elde...  \n",
       "3              2  my food stock is not the only one which is emp...  \n",
       "4              0  me, ready to go at supermarket during the #cov...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower casing\n",
    "def lower(text):\n",
    "    low_text= text.lower()\n",
    "    return low_text\n",
    "data['text']=data['text'].apply(lambda x:lower(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b4da0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>@menyrbie @phil_gahan @chrisitv  and  and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>coronavirus australia: woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>me, ready to go at supermarket during the #cov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "   Sentiment_num                                               text  \n",
       "0              1         @menyrbie @phil_gahan @chrisitv  and  and   \n",
       "1              2  advice talk to your neighbours family to excha...  \n",
       "2              2  coronavirus australia: woolworths to give elde...  \n",
       "3              2  my food stock is not the only one which is emp...  \n",
       "4              0  me, ready to go at supermarket during the #cov...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number removal\n",
    "def remove_num(text):\n",
    "    remove= re.sub(r'\\d+', '', text)\n",
    "    return remove\n",
    "data['text']=data['text'].apply(lambda x:remove_num(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e7bc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>and and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>coronavirus australia: woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>me, ready to go at supermarket during the outb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "   Sentiment_num                                               text  \n",
       "0              1                                            and and  \n",
       "1              2  advice talk to your neighbours family to excha...  \n",
       "2              2  coronavirus australia: woolworths to give elde...  \n",
       "3              2  my food stock is not the only one which is emp...  \n",
       "4              0  me, ready to go at supermarket during the outb...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove mentions and hashtags\n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "data['text']=data['text'].apply(lambda x:remove_mention(x))\n",
    "\n",
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n",
    "data['text']=data['text'].apply(lambda x:remove_hash(x))\n",
    "\n",
    "#Remove extra white space left while removing stuff\n",
    "def remove_space(text):\n",
    "    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n",
    "    return space_remove\n",
    "data['text']=data['text'].apply(lambda x:remove_space(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "005cf420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>and and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>coronavirus australia woolworths to give elder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>me ready to go at supermarket during the outbr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "   Sentiment_num                                               text  \n",
       "0              1                                            and and  \n",
       "1              2  advice talk to your neighbours family to excha...  \n",
       "2              2  coronavirus australia woolworths to give elder...  \n",
       "3              2  my food stock is not the only one which is emp...  \n",
       "4              0  me ready to go at supermarket during the outbr...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    return punct\n",
    "data['text']=data['text'].apply(lambda x:punct_remove(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2da3649a",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dr. Jayesh/nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dr. Jayesh/nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13536\\2220333227.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Remove stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mSTOPWORDS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dr. Jayesh/nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dr. Jayesh\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "data['text']=data['text'].apply(lambda x:remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8bcae",
   "metadata": {},
   "source": [
    "After all these processing, there is a possibility to end up with empty strings.Hence,we have implemented below to take care of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b8a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['text']==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23ed52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed all the text will null string in training data\n",
    "data = data[data['text']!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6850407b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    18044\n",
       "0    15397\n",
       "1     7701\n",
       "Name: Sentiment_num, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentiment_num\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b15ae",
   "metadata": {},
   "source": [
    "<h1>TF-IDF<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee10e462",
   "metadata": {},
   "source": [
    "One of the most popular approaches to counting word frequencies is TF-IDF. TF-IDF stands for Term Frequency — Inverse Document Frequency. The term frequency gives us how many times a word appears in a document whereas inverse document frequency decreases the weights of the words that occur frequently across different documents. It tells us if a word is common or rare across all documents.<br>\n",
    "\n",
    "TF-IDF is important as it helps understand the importance of a word in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c0eeb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each of the 41142 tweets is represented by 9336 features (TF-IDF score of unigrams and bigrams)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(data.text)\n",
    "\n",
    "labels = data.Sentiment_num\n",
    "\n",
    "print(\"Each of the %d tweets is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498e8c18",
   "metadata": {},
   "source": [
    "<h1>Train-test split<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16b91a",
   "metadata": {},
   "source": [
    "As the size of data is not very large, we will assign most of it to the training data. Hence using train__test_split from Scikit learn , the data has been split into 90:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d6ef2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(features,labels,test_size=0.10,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95447d9b",
   "metadata": {},
   "source": [
    "<h1>Modelling and Evaluation<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de16ce5",
   "metadata": {},
   "source": [
    "For this problem statement , I have implemented various classification models like Logistic Regression, Naive Bayes,Support Vector Machine and Random forest.<br>\n",
    "For evaluation, out of all the metrics I have used accuracy, precision and recall.<br>\n",
    "Precision: Of all the predicted positive values, how many are actually positive? This metric shows the accuracy of the model when we only look at the positive cases.<br>\n",
    "Recall: Of all the actually positive values, how many are predicted to be positive? This metric shows how well the model performs on detecting the actual positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba7cb3",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d72d33",
   "metadata": {},
   "source": [
    "The main concept of logistic regression is to use linear combinations of the observed features to estimate the particular value and the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d30d736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[12349   422  1094]\n",
      " [  624  5680   657]\n",
      " [  887   354 14960]]\n",
      "Accuracy :  0.890944445944851\n",
      "Precision :  0.8886672313370418\n",
      "Recall Score :  0.8766781471670534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89     13865\n",
      "           1       0.88      0.82      0.85      6961\n",
      "           2       0.90      0.92      0.91     16201\n",
      "\n",
      "    accuracy                           0.89     37027\n",
      "   macro avg       0.89      0.88      0.88     37027\n",
      "weighted avg       0.89      0.89      0.89     37027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LogReg = LogisticRegression(max_iter=500,multi_class='multinomial')\n",
    "LogReg.fit(xtrain, ytrain)\n",
    "pred = LogReg.predict(xtrain)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\n",
    "print(\"Accuracy : \", accuracy_score(ytrain, pred))\n",
    "print(\"Precision : \", precision_score(ytrain, pred,average='macro'))\n",
    "print(\"Recall Score : \",recall_score(ytrain, pred,average='macro'))\n",
    "print(classification_report(ytrain, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db8172eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[1247   86  199]\n",
      " [ 125  491  124]\n",
      " [ 172  107 1564]]\n",
      "Accuracy :  0.8024301336573512\n",
      "Precision :  0.7847691910618009\n",
      "Recall Score :  0.775366189415822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81      1532\n",
      "           1       0.72      0.66      0.69       740\n",
      "           2       0.83      0.85      0.84      1843\n",
      "\n",
      "    accuracy                           0.80      4115\n",
      "   macro avg       0.78      0.78      0.78      4115\n",
      "weighted avg       0.80      0.80      0.80      4115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluation on test data\n",
    "pred_test = LogReg.predict(xtest)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, pred_test))\n",
    "print(\"Accuracy : \", accuracy_score(ytest, pred_test))\n",
    "print(\"Precision : \", precision_score(ytest, pred_test,average='macro'))\n",
    "print(\"Recall Score : \",recall_score(ytest, pred_test,average='macro'))\n",
    "print(classification_report(ytest, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b840c",
   "metadata": {},
   "source": [
    "<h3>Multinomial Naive Bayes<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d660d22",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes is one of the two classic naive Bayes variants used in text classification.Naive Bayes has been considered a success many times in case of text-classification problems. Let’s see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e71bba3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[10941   100  2824]\n",
      " [ 1638  1729  3594]\n",
      " [ 1433    89 14679]]\n",
      "Accuracy:  0.738623166878224\n",
      "Precision :  0.792692233757592\n",
      "Recall Score :  0.64784943420482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78     13865\n",
      "           1       0.90      0.25      0.39      6961\n",
      "           2       0.70      0.91      0.79     16201\n",
      "\n",
      "    accuracy                           0.74     37027\n",
      "   macro avg       0.79      0.65      0.65     37027\n",
      "weighted avg       0.77      0.74      0.71     37027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MultNB = MultinomialNB()\n",
    "MultNB.fit(xtrain, ytrain)\n",
    "pred = MultNB.predict(xtrain)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\n",
    "print(\"Accuracy: \", accuracy_score(ytrain, pred))\n",
    "print(\"Precision : \", precision_score(ytrain, pred,average='macro'))\n",
    "print(\"Recall Score : \",recall_score(ytrain, pred,average='macro'))\n",
    "print(classification_report(ytrain, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7665a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[1082   18  432]\n",
      " [ 207  101  432]\n",
      " [ 239   15 1589]]\n",
      "Accuracy :  0.67363304981774\n",
      "Precision :  0.7032082524225092\n",
      "Recall Score :  0.5683113437619586\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      1532\n",
      "           1       0.75      0.14      0.23       740\n",
      "           2       0.65      0.86      0.74      1843\n",
      "\n",
      "    accuracy                           0.67      4115\n",
      "   macro avg       0.70      0.57      0.56      4115\n",
      "weighted avg       0.69      0.67      0.64      4115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluation on test data\n",
    "pred_test = MultNB.predict(xtest)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, pred_test))\n",
    "print(\"Accuracy : \", accuracy_score(ytest, pred_test))\n",
    "print(\"Precision : \", precision_score(ytest, pred_test,average='macro'))\n",
    "print(\"Recall Score : \",recall_score(ytest, pred_test,average='macro'))\n",
    "print(classification_report(ytest, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f60f9",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machine<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554c7f4",
   "metadata": {},
   "source": [
    "LinearSVC implements a “one-vs-the-rest” multi-class strategy, thus training n-class models. It is similar to SVC with parameter kernel=’ linear’.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45c24c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[12917   297   651]\n",
      " [  381  6205   375]\n",
      " [  560   264 15377]]\n",
      "Accuracy:  0.9317254976098522\n",
      "Precision :  0.9288776257351702\n",
      "Recall Score :  0.924053417989308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     13865\n",
      "           1       0.92      0.89      0.90      6961\n",
      "           2       0.94      0.95      0.94     16201\n",
      "\n",
      "    accuracy                           0.93     37027\n",
      "   macro avg       0.93      0.92      0.93     37027\n",
      "weighted avg       0.93      0.93      0.93     37027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_model = LinearSVC()\n",
    "svc_model.fit(xtrain, ytrain)\n",
    "pred = svc_model.predict(xtrain)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\n",
    "print(\"Accuracy: \", accuracy_score(ytrain, pred))\n",
    "print(\"Precision : \", precision_score(ytrain, pred,average='macro'))\n",
    "print(\"Recall Score : \",recall_score(ytrain, pred,average='macro'))\n",
    "print(classification_report(ytrain, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd87aa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[1250  101  181]\n",
      " [ 102  534  104]\n",
      " [ 165  115 1563]]\n",
      "Accuracy :  0.8133657351154313\n",
      "Precision :  0.7939246490709905\n",
      "Recall Score :  0.7952074357670863\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82      1532\n",
      "           1       0.71      0.72      0.72       740\n",
      "           2       0.85      0.85      0.85      1843\n",
      "\n",
      "    accuracy                           0.81      4115\n",
      "   macro avg       0.79      0.80      0.79      4115\n",
      "weighted avg       0.81      0.81      0.81      4115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluation on test data\n",
    "pred_test = svc_model.predict(xtest)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, pred_test))\n",
    "print(\"Accuracy : \", accuracy_score(ytest, pred_test))\n",
    "print(\"Precision : \", precision_score(ytest, pred_test,average='macro'))\n",
    "print(\"Recall Score : \",recall_score(ytest, pred_test,average='macro'))\n",
    "print(classification_report(ytest, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c8140",
   "metadata": {},
   "source": [
    "<h3>Random Forest<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13cbe2e",
   "metadata": {},
   "source": [
    "After trying with Support Vector Machine, let’s try modeling this data using the tree-based classifier RandomForestClassifier. Random forest fits several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5a9502e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[ 1704     0 12161]\n",
      " [   18     0  6943]\n",
      " [   74     0 16127]]\n",
      "Accuracy:  0.4815675047937991\n",
      "Precision :  0.46884177747473316\n",
      "Recall Score :  0.3727772558875953\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.12      0.22     13865\n",
      "           1       0.00      0.00      0.00      6961\n",
      "           2       0.46      1.00      0.63     16201\n",
      "\n",
      "    accuracy                           0.48     37027\n",
      "   macro avg       0.47      0.37      0.28     37027\n",
      "weighted avg       0.56      0.48      0.36     37027\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Jayesh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dr. Jayesh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dr. Jayesh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dr. Jayesh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "random_model = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "random_model.fit(xtrain, ytrain)\n",
    "pred = random_model.predict(xtrain)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\n",
    "print(\"Accuracy: \", accuracy_score(ytrain, pred))\n",
    "print(\"Precision : \", precision_score(ytrain, pred,average='macro'))\n",
    "print(\"Recall Score : \",recall_score(ytrain, pred,average='macro'))\n",
    "print(classification_report(ytrain, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e9003b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[ 151    0 1381]\n",
      " [   4    0  736]\n",
      " [  13    0 1830]]\n",
      "Accuracy :  0.4814094775212637\n",
      "Precision :  0.45415093239390175\n",
      "Recall Score :  0.3638367506340883\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.10      0.18      1532\n",
      "           1       0.00      0.00      0.00       740\n",
      "           2       0.46      0.99      0.63      1843\n",
      "\n",
      "    accuracy                           0.48      4115\n",
      "   macro avg       0.45      0.36      0.27      4115\n",
      "weighted avg       0.54      0.48      0.35      4115\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Jayesh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dr. Jayesh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dr. Jayesh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dr. Jayesh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#evaluation on test data\n",
    "pred_test = random_model.predict(xtest)\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, pred_test))\n",
    "print(\"Accuracy : \", accuracy_score(ytest, pred_test))\n",
    "print(\"Precision : \", precision_score(ytest, pred_test,average='macro'))\n",
    "print(\"Recall Score : \",recall_score(ytest, pred_test,average='macro'))\n",
    "print(classification_report(ytest, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167891ee",
   "metadata": {},
   "source": [
    "<h1>Summary<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7732f",
   "metadata": {},
   "source": [
    "After evaluating all the models the highest accuracy among these is achieved using Support Vector Machine and Logistic Regression with an accuracy of 81% and 80% respectively.The worst performer are Naive Bayes and Random Forest with 66% and 45% respectively.\n",
    "Let's dig deep into high performing models.\n",
    "In SVM we can see that the precision for all sentiments are above 70% which means that out of all the sentiments that the model predicted, more than 70% was correctly predicted. Similarly recall score as well is more than 70% which means that out of all the sentiments that actually predicted, the model only predicted this outcome correctly for more than 70% of those players.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74f1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
